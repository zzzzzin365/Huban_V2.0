import os
import sys
import io
import time
import wave
import numpy as np
import torch
import pyaudio
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
if sys.platform == "win32":
    sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device.upper()}")
model_name = 'iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch'
print(f"åŠ è½½æ¨¡å‹: {model_name}...")
try:
    asr_pipeline = pipeline(
        task=Tasks.auto_speech_recognition,
        model=model_name,
        device=device
    )
    print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    sys.exit(1)
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024
SILENCE_THRESHOLD = 500
SILENCE_DURATION = 1.5
MIN_SPEECH_DURATION = 0.5
def record_audio(duration=None):
    try:
        p = pyaudio.PyAudio()
        stream = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE,
            input=True,
            frames_per_buffer=CHUNK
        )
        frames = []
        silent_chunks = 0
        speech_detected = False
        start_time = time.time()
        print("\nå¼€å§‹å½•éŸ?.. (æŒ‰Ctrl+Cåœæ­¢)")
        try:
            while True:
                data = stream.read(CHUNK)
                frames.append(data)
                audio_data = np.frombuffer(data, dtype=np.int16)
                rms = np.sqrt(np.mean(audio_data ** 2))
                if rms > SILENCE_THRESHOLD:
                    silent_chunks = 0
                    speech_detected = True
                else:
                    silent_chunks += 1
                level = min(int(rms / 50), 50)
                print(f"\réŸ³é¢‘ç”µå¹³: [{'â–? * level}{' ' * (50 - level)}]", end='', flush=True)
                if duration is not None and (time.time() - start_time) >= duration:
                    break
                if duration is None and speech_detected:
                    if silent_chunks > SILENCE_DURATION * (RATE / CHUNK):
                        break
                    if (time.time() - start_time) < MIN_SPEECH_DURATION:
                        silent_chunks = 0
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å½•åˆ¶")
        finally:
            print("\nåœæ­¢å½•éŸ³")
            stream.stop_stream()
            stream.close()
            p.terminate()
            if len(frames) == 0:
                print("å½•éŸ³ä¸ºç©ºï¼Œè·³è¿‡ä¿å­?)
                return None, 0
            temp_file = "temp_recording.wav"
            wf = wave.open(temp_file, 'wb')
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(p.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
            wf.close()
            record_duration = time.time() - start_time
            if record_duration < 0.5:
                print(f"å½•éŸ³è¿‡çŸ­({record_duration:.2f}ç§?ï¼Œè¯·é‡è¯•")
                try:
                    os.remove(temp_file)
                except:
                    pass
                return None, 0
            return temp_file, record_duration
    except Exception as e:
        print(f"å½•éŸ³å¤±è´¥: {str(e)}")
        return None, 0
def transcribe_audio(audio_path):
    if not audio_path or not os.path.exists(audio_path):
        return None, 0
    try:
        start_time = time.time()
        result = asr_pipeline(audio_in=audio_path, param_dict={"decoding_model": "normal"})
        text = result.get('text', '')
        elapsed = time.time() - start_time
        return text, elapsed
    except Exception as e:
        print(f"è¯†åˆ«é”™è¯¯: {str(e)}")
        return None, 0
def real_time_asr():
    print("=" * 70)
    print(f"{'ä¸­æ–‡å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ':^70}")
    print(f"{'åŸºäºParaformeræ¨¡å‹':^70}")
    print(f"{'æŒ‰å›è½¦å¼€å§‹å½•éŸ³ï¼ŒæŒ‰Ctrl+Cé€€å‡ºç³»ç»?:^70}")
    print("=" * 70)
    try:
        while True:
            try:
                input("æŒ‰å›è½¦é”®å¼€å§‹è¯´è¯?..")
            except UnicodeDecodeError:
                if hasattr(sys.stdin, 'buffer') and hasattr(sys.stdin.buffer, 'in_waiting'):
                    sys.stdin.buffer.read(sys.stdin.buffer.in_waiting)
                print("è¾“å…¥ç¼–ç é—®é¢˜ï¼Œè¯·é‡è¯•...")
                continue
            audio_file, record_duration = record_audio()
            if not audio_file:
                print("å½•éŸ³å¤±è´¥ï¼Œè¯·é‡è¯•")
                continue
            print("å¼€å§‹è¯†åˆ?..")
            transcription, process_time = transcribe_audio(audio_file)
            rtf = process_time / record_duration if record_duration > 0 else 0
            print("\n" + "=" * 70)
            print(f"å½•éŸ³æ—¶é•¿: {record_duration:.2f}ç§?)
            print(f"å¤„ç†æ—¶é—´: {process_time:.2f}ç§?(RTF: {rtf:.2f})")
            print(f"è¯†åˆ«ç»“æœ:")
            print("-" * 70)
            print(transcription if transcription else "æœªè¯†åˆ«åˆ°æœ‰æ•ˆå†…å®¹")
            print("=" * 70)
            try:
                os.remove(audio_file)
            except:
                pass
            if device.startswith('cuda'):
                mem_used = torch.cuda.memory_allocated(0) / 1024 ** 2
                mem_total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 2
                print(f"GPUå†…å­˜ä½¿ç”¨: {mem_used:.2f}/{mem_total:.2f} MB")
            print("\nå‡†å¤‡ä¸‹ä¸€æ¬¡è¯†åˆ?..\n")
    except KeyboardInterrupt:
        print("\né€€å‡ºç³»ç»?)
if __name__ == "__main__":
    real_time_asr()
